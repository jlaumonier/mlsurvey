from multiprocessing import Pool

from tqdm import tqdm

import mlsurvey as mls
from .learning_workflow import LearningWorkflow


class MultipleLearningWorkflow(LearningWorkflow):

    def __init__(self, config_file='config.json', config_directory='config/', base_directory='', ):
        """
        Initialized the multiple learning workflow
        :param config_file: config file for initializing the workflow, Used if config is None
        """
        super().__init__(config_directory=config_directory, base_directory=base_directory)
        self.task_terminated_expand_config = False
        self.task_terminated_run_each_config = False
        try:
            self.config = mls.Config(config_file, directory=self.config_directory)
        except (FileNotFoundError, mls.exceptions.ConfigError):
            self.task_terminated_init = False
        self.expanded_config = []
        self.slw = []

    def set_terminated(self):
        """ set the workflow as terminated if all tasks are terminated"""
        self.terminated = (self.task_terminated_init
                           & self.task_terminated_expand_config
                           & self.task_terminated_run_each_config)

    def task_expand_config(self):
        """
        Get a configuration file with lists in parameters and hyperparameter for datasets, algorithms and splits
        and generate a list containing the cartesian product of all possible configs.
        :return: a list of dictionary, each one is a config usable by a supervised learning workflow
        """
        for lp_element in list(mls.Utils.dict_generator_cartesian_product(self.config.data['learning_process'])):
            input_parameters = self.config.data['datasets'][lp_element['input']]['parameters']
            if 'fairness' in self.config.data['datasets'][lp_element['input']]:
                fairness_parameters = self.config.data['datasets'][lp_element['input']]['fairness']
            else:
                fairness_parameters = {}
            for ds_element in list(mls.Utils.dict_generator_cartesian_product(input_parameters)):
                for fp_element in list(mls.Utils.dict_generator_cartesian_product(fairness_parameters)):
                    one_dataset = dict()
                    one_dataset[lp_element['input']] = self.config.data['datasets'][lp_element['input']].copy()
                    one_dataset[lp_element['input']]['parameters'] = ds_element
                    if fairness_parameters != {}:
                        one_dataset[lp_element['input']]['fairness'] = fp_element
                    algorithm_hyperparameters = self.config.data['algorithms'][lp_element['algorithm']][
                        'hyperparameters']
                    for alg_element in list(mls.Utils.dict_generator_cartesian_product(algorithm_hyperparameters)):
                        one_algo = dict()
                        one_algo[lp_element['algorithm']] = self.config.data['algorithms'][
                            lp_element['algorithm']].copy()
                        one_algo[lp_element['algorithm']]['hyperparameters'] = alg_element
                        split_parameters = self.config.data['splits'][lp_element['split']]['parameters']
                        for split_element in list(mls.Utils.dict_generator_cartesian_product(split_parameters)):
                            one_split = dict()
                            one_split[lp_element['split']] = self.config.data['splits'][lp_element['split']].copy()
                            one_split[lp_element['split']]['parameters'] = split_element
                            one_config = {'datasets': one_dataset,
                                          'algorithms': one_algo,
                                          'splits': one_split,
                                          'learning_process': lp_element}
                            self.expanded_config.append(one_config)
        self.task_terminated_expand_config = True

    @staticmethod
    def task_run_one_config(p):
        """
        Run one supervised learning workflow
        :param p : tupple parameter containing
                    c: configuration for the supervised learning workflow
                    bd: base directory for all file loaded with config
        :return: the supervised learning workflow after a run
        """
        c = p[0]
        bd = p[1]
        sl = mls.workflows.SupervisedLearningWorkflow(config=c, base_directory=bd)
        sl.run()
        return sl

    def task_run_each_config(self):
        """
        Run each config with a supervised learning workflow generated by task_expand_config()
        """

        def dataset_storage(c):
            """ get the storage from a dataset in the config dictionary"""
            result = 'Pandas'
            if 'storage' in c['datasets'][c['learning_process']['input']]:
                result = c['datasets'][c['learning_process']['input']]['storage']
            return result

        configs_pandas = [(c, self.base_directory) for c in self.expanded_config if dataset_storage(c) == 'Pandas']
        configs_dask = [(c, self.base_directory) for c in self.expanded_config if dataset_storage(c) == 'Dask']
        # Only pandas with pool
        print('Pandas execution')
        pool = Pool()
        with tqdm(total=len(configs_pandas)) as pbar:
            enumr = enumerate(pool.imap_unordered(MultipleLearningWorkflow.task_run_one_config,
                                                  configs_pandas))
            for i, res in tqdm(enumr):
                self.slw.append(res)
                pbar.update()
        pool.close()
        pool.join()

        # Dask is not launched with multithread.Pool()
        print('Dask execution')
        for i, params in enumerate(configs_dask):
            res = MultipleLearningWorkflow.task_run_one_config(params)
            self.slw.append(res)
            print(str(i + 1) + '/' + str(len(configs_dask)))

        self.task_terminated_run_each_config = True

    def run(self):
        """
        Run the workflow only if the initialization was done correctly:
        - Expand configs
        - run each config
        - terminated the workflow
        """
        if self.task_terminated_init:
            self.task_expand_config()
            self.task_run_each_config()
            self.set_terminated()
